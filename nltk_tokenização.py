# -*- coding: utf-8 -*-
"""NLTK - Tokenização

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EnOdiw0aEorXuReARPQuXQklhgbYNcHN
"""

import nltk #importando a biblioteca NLTK
nltk.download('machado') #Fazendo o download do corpus caso esse não tenha sido previamente baixado
nltk.download('punkt') #Biblioteca utilizada para tokerizar as sentenças
nltk.download('punkt_tab') # Downloading punkt_tab for Portuguese
from nltk.corpus import machado #Importando o corpus machado

print(machado.categories())

import nltk
nltk.download('mac_morpho')
from nltk.corpus import mac_morpho

# O corpus inteiro vem tokenizado por palavras, precisamos juntar:
texto_completo = " ".join([" ".join(sentenca) for sentenca in mac_morpho.sents()])

print(texto_completo[:2000])  # Mostra apenas os primeiros 2000 caracteres para não travar

from nltk.tokenize import word_tokenize # importando o tokerizador de palavras
from nltk.tokenize import sent_tokenize # importando o tokerizador de sentenças

#Criando um texto de exemplo
texto = 'Gosto de análise de dados combinado com programação e sou formada em Tecnologia da Informação, desejo firmar em dados.'

nltk.sent_tokenize(texto) # Tokenizando um texto considerando sentenças como unidades

word_tokenize(texto) # Tokenizando um texto considerando palavras como unidades

from nltk.tokenize import TweetTokenizer # importando o tokerizador de tweets
texto2 = "Eu estou felizzzzz em aprender sobre processamento de linguagem natural #NLP @univesp :)"

word_tokenize(texto2) # Tokenizando um texto considerando palavras como unidades

twitterTokenizer = TweetTokenizer()
twitterTokenizer.tokenize(texto2) # Tokenizando um texto considerando palavras como unidades

twitterTokenizer2 = TweetTokenizer(strip_handles=True, reduce_len=True) # strip_handles remove os nomes de usuário do texto / reduce_len substitui sequencias de 3 ou mais caracteteres repetidos por sequência de 3 caracteres
twitterTokenizer2.tokenize(texto2)

import nltk
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('maxent_ne_chunker_tab') # Added download for maxent_ne_chunker_tab

texto2= 'I currently work at Carrefour focusing on market intelligence.'

print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(texto2)))) # a cada token mostra oq significa em inglês

import spacy # importando a biblioteca spacy

import spacy

# Load a SpaCy English language model
# You might need to run a separate cell to download this model first:
# !python -m spacy download en_core_web_sm
nlp = spacy.load('en_core_web_sm')

setenca = nlp ('Andre is working at Google in the South America \ He works very hard')
for token in setenca: # A propriedade .is_stop retorna True se o Token está
  print(f'{token.text} - {token.is_stop}') # na lista de stopwords

setenca = nlp('Andre is working at Google in the South America since 1999. \ He works very hard! This product costs $90') # A propriedade .is_alpha retorna True se o token é composto apenas por caracteres alfabéticos
for token in setenca:
  print(f'{token.text} - {token.is_alpha}')